
## <img src="assets/belle_logo.png" style="vertical-align: middle; width: 35px;"> BELLE: Be Everyone's Large Language model Engine

*[ä¸­æ–‡README](README.md).*

<div align="center">

<a href="https://github.com/LianjiaTech/BELLE/stargazers">![GitHub Repo stars](https://img.shields.io/github/stars/LianjiaTech/BELLE?style=social)</a>
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/LianjiaTech/BELLE/blob/main/LICENSE)
[![Generic badge](https://img.shields.io/badge/discord-BELLE%20Group-green.svg?logo=discord)](https://discord.gg/pMPY53UUGq)
[![Generic badge](https://img.shields.io/badge/wechat-BELLE-green.svg?logo=wechat)](https://github.com/LianjiaTech/BELLE/blob/main/assets/belle_wechat.jpg)
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/BelleGroup)
<a href="https://github.com/LianjiaTech/BELLE/tree/main/docs/">![Docs](https://img.shields.io/badge/papers-BELLE%2Fdocs-green)</a>
<a href="https://github.com/LianjiaTech/BELLE/tree/main/gptq/">![Docs](https://img.shields.io/badge/quantization_recipe-BELLE%2Fgptq-green)</a>
<a href="https://github.com/LianjiaTech/BELLE/tree/main/train/">![Docs](https://img.shields.io/badge/train_recipe-BELLE%2Ftrain-green)</a>
<a href="https://github.com/LianjiaTech/BELLE/tree/main/eval/">![Docs](https://img.shields.io/badge/eval_set-BELLE%2Feval-green)</a>
<a href="https://github.com/LianjiaTech/BELLE/tree/main/chat/">![Docs](https://img.shields.io/badge/ChatBELLE-BELLE%2Fchat-green)</a>

</div>

The goal of this project is to promote the development of the open-source community for Chinese language large-scale conversational models, and our vision is to help building large language model engine for everyone. This project optimizes Chinese performance based on opensource pretrained large language models. These models finetuning uses only data generated via ChatGPT (without other data). 
<br/>

## ChatBELLE App

Try our cross-platform chat app to run 4-bit quantized BELLE-7B model natively on your device.
The following screencap ran on an M1 Max CPU real-time (no speed adjustment).

**App Downloading**ï¼šReleases

[App Companion Model and Usage](chat/README.md)

<img src="./chat/chatbelle-demo.gif"></img>

## ğŸ”„ Whatâ€˜s new

* [2023/04/12] Released [ChatBELLE App](chat/README.md), a cross-platform BELLE-7B model realtime chat App based on [llama.cpp](https://github.com/ggerganov/llama.cpp) and [Flutter](https://flutter.dev/).
* [2023/04/08] In [BELLE/10M](https://github.com/LianjiaTech/BELLE/tree/main/10M), a new dataset named ["Generated Chat"]((https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)) containing newly generated multi-turn dialogues with given roles, and a new dataset named ["train_2M_CN"](https://huggingface.co/datasets/BelleGroup/train_2M_CN) containing 2 million newly added diverse instruction task data.
* [2023/04/05] The inference code that can be run on [Colab](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/notebook/BELLE_INFER_COLAB.ipynb) is provided

## ğŸ“ This repo contains

###  ğŸš€ Traning recipe

  Details in [BELLE/train](train/), A simplified implementation of training code with support for finetune, LORA, and DeepSpeed as much as possible.

### ğŸ“Š Data Release
  
  Details in [BELLE/1.5M](1.5M/)ï¼ŒThe Chinese dataset generated [1M](https://huggingface.co/datasets/BelleGroup/generated_train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN), using [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) as reference
  
  10M more data will be released graduallyï¼Œdetails in [BELLE/10M](10M/). Currently, we have 0.8M multiturn data, and 0.25 math data.

### ğŸ§ Evaluation set & evaluation method
  
  Details in [BELLE/eval](eval/). A test set with over 1k samples and corresponding scoring prompts. It includes multiple categories and is evaluated using either GPT-4 or ChatGPT.

### ğŸ¤– Models

  Details in [BELLE/models](models/)
  
* The model optimized based on BLOOMZ-7B1-mtï¼š[BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M)ï¼Œ[BELLE-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-7B-0.6M)ï¼Œ[BELLE-7B-1M](https://huggingface.co/BelleGroup/BELLE-7B-1M)ï¼Œ[BELLE-7B-2M](https://huggingface.co/BelleGroup/BELLE-7B-2M)
  
* The finetuned models based on [Meta LLaMA](https://github.com/facebookresearch/llama): [BELLE-LLaMA-7B-0.6M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-0.6M-enc)
, [BELLE-LLaMA-7B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-enc)
, [BELLE-LLaMA-7B-2M-gptq-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-gptq-enc)
, [BELLE-LLaMA-13B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc). Considering [LLaMA's License](https://github.com/facebookresearch/llama/blob/main/LICENSE) constraints, the model is for research and learning only. Please strictly respect LLaMA's usage policy. Users are suggested to finetune the model with open-source scripts and datasets. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. You can find the decrypt code on [BELLE/models](models/).

### âš–ï¸ Quantized_models

  Details in [BELLE/gptq](gptq/)ï¼ŒReferring to the implementation of GPT-Q, the relevant models in this project have been quantized.

### ğŸŒ Colab
  
  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/notebook/BELLE_INFER_COLAB.ipynb) provides the colab in [BELLE/notebook](https://colab.research.google.com/github/LianjiaTech/BELLE/blob/main/notebook/BELLE_INFER_COLAB.ipynb)

### ğŸ’¬ ChatBELLE App

  Details in [BELLE/chat](chat/README.md), cross-platform LLM chat app with [BELLE](https://github.com/LianjiaTech/BELLE) using quantized on-device offline models and Flutter UI, running on macOS (done), Windows, Android, iOS and more.

**More prompts are welcomed via issues!**

<br/>

## âš ï¸ Limitation and Usage Limits

There still exists a few issues in the model trained on current base model and data:

1. The model might generate factual errors when asked to follow instructions related to facts.

2. Occasionally generates harmful responses since the model still struggles to identify potential harmful instructions.

3. Needs improvements on reasoning and coding.

Since the model still has its limitations, we require developers only use the open-sourced code, data, model and any other artifacts generated via this project for research purposes. Commercial use and other potential harmful use cases are not allowed.

<br/>

## ğŸ“Œ Citation

Please cite us when using our code, data or model.

```
@misc{BELLE,
  author = {Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma and Xiangang Li},
  title = {BELLE: Be Everyone's Large Language model Engine},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/LianjiaTech/BELLE}},
}

@article{belle2023exploring,
  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},
  author={Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, Xiangang Li},
  journal={arXiv preprint arXiv:2303.14742},
  year={2023}
}
```

Cite the original BLOOM, LLaMA, Stanford Alpaca and Self-Instruct papers as well!

</br>

## ğŸ“š Use case
| Human | Assitant |
| ----- | ------ |
| å†™ä¸€ç¯‡è‹±æ–‡æ•£æ–‡è¯—ï¼Œä¸»é¢˜æ˜¯æ˜¥é›¨ï¼Œæƒ³è±¡è‡ªå·±æ˜¯æ˜¥é›¨ï¼Œå’Œè‹±å›½å¤ä»£è¯—äººèå£«æ¯”äºšäº¤æµ | I am the spring rain,<br/>Dancing on your face and body.<br/>My dew-drenched petals dance with you,<br/>Bringing life to every plant.<br/><br/>Shakespeare, I am here to meet you.<br/>Your works have filled my heart with joy.<br/>The beauty of nature in its fullness<br/>Makes me feel like a partaker.<br/><br/>As the leaves sway and rustle by,<br/>My dew-filled hand reaches out for them.<br/>The sweet scent of flowers fill the air,<br/>And bring me peace from all around.<br/><br/>Butterflies flit about as well,<br/>Glistening in the sun's rays.<br/>Their wings are made of dew,<br/>A beautiful sight to behold.<br/><br/>You see, Shakespeare, I too am one with nature.<br/>My dew-drenched hair is a symbol of hope.<br/>For when we can connect back to our roots,<br/>We will be able to make it through this season. |
| I am the spring rain,<br/>Dancing on your face and body.<br/>My dew-drenched petals dance with you,<br/>Bringing life to every plant.<br/><br/>Shakespeare, I am here to meet you.<br/>Your works have filled my heart with joy.<br/>The beauty of nature in its fullness<br/>Makes me feel like a partaker.<br/><br/>As the leaves sway and rustle by,<br/>My dew-filled hand reaches out for them.<br/>The sweet scent of flowers fill the air,<br/>And bring me peace from all around.<br/><br/>Butterflies flit about as well,<br/>Glistening in the sun's rays.<br/>Their wings are made of dew,<br/>A beautiful sight to behold.<br/><br/>You see, Shakespeare, I too am one with nature.<br/>My dew-drenched hair is a symbol of hope.<br/>For when we can connect back to our roots,<br/>We will be able to make it through this season. <br/>æŠŠä¸Šé¢çš„æ•£æ–‡è¯—ç¿»è¯‘æˆä¸­æ–‡ | æˆ‘æ˜¯ä¸€åœºæ˜¥é›¨ï¼Œ<br/>åœ¨ä½ è„¸ä¸Šè·³è·ƒã€‚<br/>æˆ‘æµ¸æ¶¦ç€éœ²ç çš„ç¿…è†€å’Œä½ ä¸€èµ·èˆåŠ¨ï¼Œ<br/>ä¸ºæ¯ä¸€æ ªæ¤ç‰©å¸¦æ¥ç”Ÿå‘½ã€‚<br/><br/>èå£«æ¯”äºšï¼Œæˆ‘æ¥ä¸ä½ ç›¸é‡ã€‚<br/>ä½ çš„ä½œå“å¡«æ»¡äº†æˆ‘çš„å¿ƒæˆ¿ã€‚<br/>å¤§è‡ªç„¶çš„ç¾ä¸½è®©æˆ‘æ„Ÿåˆ°ä¸ä¹‹ä¸€èµ·åˆ†äº«ï¼Œ<br/>è®©æˆ‘è§‰å¾—è‡ªå·±æˆä¸ºäº†å…¶ä¸­çš„ä¸€éƒ¨åˆ†ã€‚<br/><br/>å½“æ ‘å¶æ‘‡æ›³æ—¶ï¼Œ<br/>æˆ‘çš„æ‰‹æŒæ²¾æ»¡äº†éœ²æ°´ã€‚<br/>èŠ±é¦™å¼¥æ¼«åœ¨ç©ºæ°”ä¸­ï¼Œ<br/>å¸¦ç»™æˆ‘å®é™å’Œå®‰å®ã€‚<br/><br/>ä½ çœ‹ï¼Œèå£«æ¯”äºšï¼Œæˆ‘ä¹Ÿæ˜¯ä¸å¤§è‡ªç„¶èä¸ºä¸€ä½“çš„ã€‚<br/>æˆ‘æ²¾æ¹¿çš„å¤´å‘è±¡å¾ç€å¸Œæœ›ã€‚<br/>å½“æˆ‘ä»¬èƒ½å¤Ÿå›åˆ°æˆ‘ä»¬çš„æ ¹æºï¼Œ<br/>æˆ‘ä»¬å°±èƒ½å…±åŒåº¦è¿‡è¿™ä¸ªå­£èŠ‚ã€‚|
| Classify the sentiment of the following sentence into Positive, Neutral, or Negative: <br/>Super excited about teaching Stanfordâ€™s first course on Large Language Models! Check the syllabus out here | Positive |
| å°æ˜çš„çˆ¸çˆ¸æœ‰ä¸‰ä¸ªå­©å­ï¼Œè€å¤§å«ç‹ä¸€ï¼Œè€äºŒå«ç‹äºŒï¼Œè€ä¸‰å«ä»€ä¹ˆï¼Ÿ | å°æ˜ |
| ä½¿ç”¨pythonå†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾çš„ä»£ç  | def binary_search(arr, target):<br/>&nbsp;&nbsp;low = 0<br/>&nbsp;&nbsp;high = len(arr) - 1<br/>&nbsp;&nbsp;while (low <= high):<br/>&nbsp;&nbsp;&nbsp;&nbsp;mid = low + high // 2<br/>&nbsp;&nbsp;&nbsp;&nbsp;if arr[mid] == target:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return mid<br/>&nbsp;&nbsp;&nbsp;&nbsp;elif target < arr[mid]:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;low = mid + 1<br/>&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;high = mid - 1<br/>&nbsp;&nbsp;return -1 |
| ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼ŒæŠŠè¿™å¥è¯ç¿»è¯‘æˆè‹±è¯­ | What's the weather like today? |
| æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®® | ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥è®©ä½ ä¿æŒç²¾åŠ›å……æ²›çš„å»ºè®®ï¼š <br/>1. ç¡çœ å……è¶³ã€‚ç¡®ä¿æ¯å¤©æ™šä¸Šå¾—åˆ°å……è¶³çš„ç¡çœ æ—¶é—´ã€‚ <br/>2. å¤šè¿åŠ¨ã€‚è¿›è¡Œé€‚åº¦çš„æœ‰æ°§è¿åŠ¨ï¼Œå¦‚æ…¢è·‘ã€æ¸¸æ³³ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ æé«˜èƒ½é‡å’Œè€åŠ›ã€‚<br/>3. åƒå¥åº·é£Ÿå“ã€‚é€‰æ‹©å¯Œå«è›‹ç™½è´¨çš„é£Ÿç‰©ï¼Œå¦‚é¸¡è‚‰ã€é±¼è‚‰å’Œé¸¡è›‹ï¼Œä»¥åŠæ–°é²œçš„æ°´æœå’Œè”¬èœã€‚ <br/>4. å‡å°‘å’–å•¡å› çš„æ‘„å…¥é‡ã€‚å°½é‡é¿å…åœ¨ç™½å¤©å–å«å’–å•¡å› çš„é¥®æ–™æˆ–åƒå«æœ‰å’–å•¡å› çš„é£Ÿç‰©ã€‚ <br/>5. æ”¾æ¾èº«å¿ƒã€‚å°è¯•å†¥æƒ³ã€ç‘œä¼½æˆ–å…¶ä»–å½¢å¼çš„æ”¾æ¾æ´»åŠ¨æ¥å‡è½»å‹åŠ›å’Œç„¦è™‘æ„Ÿã€‚ |
| æ˜å¤©å°±å‡æœŸç»“æŸäº†ï¼Œæœ‰ç‚¹æŠ—æ‹’ä¸Šç­ï¼Œåº”è¯¥ä»€ä¹ˆåŠï¼Ÿ | é¢å¯¹å³å°†ç»“æŸçš„å‡æœŸå’Œå¼€å§‹å·¥ä½œæ—¶çš„ç´§å¼ æƒ…ç»ªï¼Œæ‚¨å¯ä»¥é‡‡å–ä»¥ä¸‹æªæ–½æ¥å¸®åŠ©æ‚¨æ›´å¥½åœ°é€‚åº”ï¼š <br/>1. ä¸ºè‡ªå·±åˆ¶å®šä¸€ä¸ªæ¸…æ™°çš„å·¥ä½œè®¡åˆ’ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨æ›´å¥½åœ°ç»„ç»‡æ—¶é—´å’Œä»»åŠ¡ï¼Œå¹¶é¿å…åœ¨å·¥ä½œä¸­æ„Ÿåˆ°æ··ä¹±ã€‚ <br/>2. ä¸åŒäº‹æˆ–æœ‹å‹äº¤æµï¼Œåˆ†äº«æ‚¨çš„æ„Ÿå—å’Œå·¥ä½œå‹åŠ›ã€‚è¿™å°†æœ‰åŠ©äºç¼“è§£ä¸€äº›ç„¦è™‘æ„Ÿï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥è·å¾—æ”¯æŒå’Œå»ºè®®ã€‚ <br/>3. åœ¨å·¥ä½œæ—¥å‰èŠ±äº›æ—¶é—´æ”¾æ¾èº«å¿ƒï¼Œä¾‹å¦‚è¿›è¡Œç‘œä¼½ã€å†¥æƒ³ç­‰æ´»åŠ¨ã€‚è¿™äº›æ–¹æ³•å¯ä»¥å‡è½»èº«ä½“å’Œç²¾ç¥ä¸Šçš„ç–²åŠ³æ„Ÿï¼Œä½¿æ‚¨æ›´åŠ ç²¾åŠ›å……æ²›åœ°è¿æ¥æŒ‘æˆ˜ã€‚ |
| çˆ¶æ¯éƒ½å§“å´ï¼Œå–ä¸€äº›ç”·å®å®å’Œå¥³å®å®çš„åå­— | - ç”·å®ï¼šå´ä¿Šæ°ã€å´é›¨è¾°ã€å´å­å¢¨ã€å´æ€æºã€å´æ³½é˜³ã€å´å®‡èˆªã€å´åšæ–‡ã€å´å“²æ¯…ã€‚ <br/>- å¥³å®ï¼šå´ç¾çªã€å´ä½³æ€¡ã€å´æ¬£ç‘œã€å´é¦™å‡ã€å´é›ªæŸ”ã€å´é¢–è¯—ã€å´è¯­å«£ã€‚ |
| æ¨èå‡ æœ¬é‡‘åº¸çš„æ­¦ä¾ å°è¯´ | ä»¥ä¸‹æ˜¯ä¸‰æœ¬æˆ‘æœ€å–œæ¬¢çš„é‡‘åº¸æ­¦ä¾ å°è¯´çš„æ¨èï¼š <br/>1. ã€Šå°„é›•è‹±é›„ä¼ ã€‹ - ä¸€éƒ¨å……æ»¡æ™ºæ…§å’Œæœºæ™ºçš„å°è¯´ï¼Œè®²è¿°äº†éƒ­é–ã€é»„è“‰å¤«å¦‡çš„æ•…äº‹ã€‚ <br/>2. ã€Šç¥é›•ä¾ ä¾£ã€‹- è¿™æ˜¯ä¸€éƒ¨æµªæ¼«è€Œæ„Ÿäººçš„æ•…äº‹ï¼Œè®²è¿°äº†ä¸€å¯¹æƒ…ä¾£å°é¾™å¥³ä¸æ¨è¿‡ä¹‹é—´çš„çˆ±æƒ…æ•…äº‹ã€‚ <br/>3. ã€Šå€šå¤©å± é¾™è®°ã€‹- è¿™æ˜¯ä¸€ä¸ªå®ä¼Ÿè€Œå£®è§‚çš„æ•…äº‹ï¼Œæç»˜äº†æ˜æ•™é¢†è¢–å¼ æ— å¿Œå¦‚ä½•æˆä¸ºä¸€ä½å‡ºè‰²çš„è‹±é›„ã€‚ |

<br/>

