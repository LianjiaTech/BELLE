
## ğŸ“Belle-VL
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo2-green.svg)](https://huggingface.co/BELLE-2/BELLE-VL)
### èƒŒæ™¯ä»‹ç»
ç¤¾åŒºç›®å‰å·²ç»æœ‰å¾ˆå¤šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸å…³å¼€æºå·¥ä½œï¼Œä½†å¤§å¤šä»¥è‹±æ–‡èƒ½åŠ›ä¸ºä¸»ï¼Œæ¯”å¦‚[LLava](https://github.com/haotian-liu/LLaVA),[CogVLM](https://github.com/THUDM/CogVLM)ç­‰ï¼Œè€Œä¸­æ–‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¯”å¦‚[VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B)ã€[Qwen-VL](https://github.com/QwenLM/Qwen-VL)çš„è¯­è¨€æ¨¡å‹åŸºåº§å‡è¾ƒå°ï¼Œå®é™…åº”ç”¨ä¸­å¾ˆéš¾å…¼é¡¾è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œå› æ­¤Belle-VLé€‰æ‹©åŸºäºæ›´å¼ºçš„è¯­è¨€æ¨¡å‹åŸºåº§æ¥æ‰©å±•æ¨¡å‹çš„è§†è§‰èƒ½åŠ›ï¼Œä¸ºç¤¾åŒºæä¾›æ›´åŠ çµæ´»çš„é€‰æ‹©ã€‚

### æ¨¡å‹ç®€ä»‹
åœ¨æ¨¡å‹ç»“æ„æ–¹é¢ï¼Œæˆ‘ä»¬ä¸»è¦å‚è€ƒçš„[Qwen-VL](https://github.com/QwenLM/Qwen-VL)æ¨¡å‹ï¼ŒåŸå§‹Qwen-VLæ˜¯åŸºäºQwen7Bæ¨¡å‹è®­ç»ƒè€Œæ¥ï¼ŒåŸºåº§èƒ½åŠ›ç›¸å¯¹è¾ƒå¼±ï¼Œå› æ­¤Belle-VLå°†è¯­è¨€æ¨¡å‹æ‰©å±•æˆäº†[Qwen14B-chat](https://huggingface.co/Qwen/Qwen-14B-Chat)ï¼Œåœ¨ä¸­æ–‡è¯­è¨€èƒ½åŠ›å’Œè§†è§‰èƒ½åŠ›æ–¹é¢å¯ä»¥å…¼é¡¾ï¼Œå…·å¤‡æ›´å¥½çš„æ‰©å±•æ€§ã€‚

### è®­ç»ƒç­–ç•¥
åŸå§‹Qwen-vlé‡‡ç”¨äº†ä¸‰é˜¶æ®µçš„è®­ç»ƒæ–¹å¼,åŒ…æ‹¬é¢„è®­ç»ƒã€å¤šä»»åŠ¡è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒï¼Œä¾èµ–è¾ƒå¤§çš„æ•°æ®å’Œæœºå™¨èµ„æºã€‚å—LLava1.5çš„å¯å‘ï¼Œå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ¯”é¢„è®­ç»ƒæ›´åŠ é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![Traing_stage](./train.png)

### è®­ç»ƒæ•°æ®
* é¢„è®­ç»ƒæ•°æ®ï¼šé¢„è®­ç»ƒæ•°æ®ä¸»è¦æ˜¯åŸºäºLLava çš„[558k](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain)è‹±æ–‡æŒ‡ä»¤æ•°æ®åŠå…¶å¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘æ•°æ®ï¼Œæ­¤å¤–æˆ‘ä»¬è¿˜æ”¶é›†äº†[Flickr30k-CNA](https://zero.so.com/) ä»¥åŠä»[AI Challenger](https://tianchi.aliyun.com/dataset/145781?spm=a2c22.12282016.0.0.5c823721PG2nBW)éšæœºé€‰å–çš„100kæ•°æ®

* å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ï¼šæŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæ•°æ®ä¸»è¦æ¥è‡ª[LLava](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR),[LVIS-INSTRUCT4V](https://github.com/X2FD/LVIS-INSTRUCT4V)ç­‰å¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹å…¶ä¸­éƒ¨åˆ†æ•°æ®è¿›è¡Œäº†ç¿»è¯‘ï¼Œåœ¨æ­¤çœŸè¯šçš„æ„Ÿè°¢ä»–ä»¬ä¸ºå¼€æºæ‰€åšå‡ºçš„è´¡çŒ®ï¼

### æ¨¡å‹ä½¿ç”¨
``` python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
model_dir = '/path/to_finetuned_model/'
img_path = 'you_image_path'
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True).eval()
model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)
question = 'è¯¦ç»†æè¿°ä¸€ä¸‹è¿™å¼ å›¾'

query = tokenizer.from_list_format([
    {'image': img_path}, # Either a local path or an url
    {'text': question},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)

#or
query = f'<img>{img_path}</img>\n{question}'
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
```

### MME Benchmark
[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°åŸºå‡†ã€‚å®ƒåœ¨æ€»å…±14ä¸ªå­ä»»åŠ¡ä¸Šæµ‹é‡æ„ŸçŸ¥å’Œè®¤çŸ¥èƒ½åŠ›,åŒ…æ‹¬
åŒ…æ‹¬å­˜åœ¨æ€§ã€è®¡æ•°ã€ä½ç½®ã€é¢œè‰²ã€æµ·æŠ¥ã€åäººã€åœºæ™¯ã€åœ°æ ‡ã€è‰ºæœ¯ä½œå“ã€OCRã€å¸¸è¯†æ¨ç†ã€æ•°å€¼è®¡ç®—ã€æ–‡æœ¬ç¿»è¯‘å’Œä»£ç æ¨ç†ç­‰ã€‚BELLE-VLåœ¨æ„ŸçŸ¥è¯„æµ‹å…±è·å¾—1595.34åˆ†ï¼Œè¶…è¿‡LLavaå’ŒQwen-VL.è¯¦æƒ…å¦‚ä¸‹ï¼š
| Category               | Score |
|------------------------|-------|
| **Perception**         | **1595.34**    |
| --Existence              | 190   |
| --Count                  | 150   |
| --Position               | 130   |
| --Color                  | 175   |
| --Posters                | 166.33|
| --Celebrity              | 136.76|
| --Scene                  | 156.25|
| --Landmark               | 174   |
| --Artwork                | 139.5 |
| --OCR                    | 177.5 |

| Category               | Score |
|------------------------|-------|
| **Cognition**          | **332.14**    |
| --Commonsense Reasoning   | 127.14|
| --Numerical Calculation  | 47.5  |
| --Text Translation       | 102.5 |
| --Code Reasoning         | 55    |



